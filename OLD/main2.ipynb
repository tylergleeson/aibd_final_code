{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 1. CONFIGURATION & UTILS\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column settings\n",
    "ID_COL = 'SKU'               # Entity identifier for panel\n",
    "DATE_COL = 'Date'            # Date column\n",
    "TARGET = 'orders'            # Forecast target\n",
    "\n",
    "# Exogenous features to carry through from raw data\n",
    "EXOG_FEATURES = [\n",
    "    'tempmax', 'tempmin', 'temp', 'humidity',\n",
    "    'precip', 'windspeedmean', 'Fuel_Price',\n",
    "    'Median_HH_Income', 'Unemployment_Rate'\n",
    "]\n",
    "\n",
    "# Custom scoring scaffold\n",
    "def custom_cost_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom cost-based scoring function (negative for maximization).\n",
    "    Replace this logic with business-specific cost calculations.\n",
    "    \"\"\"\n",
    "    cost = np.abs(y_true - y_pred)\n",
    "    return -np.sum(cost)\n",
    "\n",
    "custom_scorer = make_scorer(custom_cost_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 2. DATA LOADING & AGGREGATION\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_aggregate(path: str) -> pd.DataFrame:\n",
    "    # Load raw transactional data\n",
    "    df = pd.read_excel(path, parse_dates=[DATE_COL])\n",
    "    # Aggregate to panel time series: count unique transactions per SKU per day\n",
    "    aggs = { 'Transaction_ID': 'nunique' }\n",
    "    # include exogenous first-values\n",
    "    for col in EXOG_FEATURES:\n",
    "        aggs[col] = 'first'\n",
    "    panel = (\n",
    "        df.groupby([ID_COL, DATE_COL])\n",
    "          .agg(aggs)\n",
    "          .reset_index()\n",
    "          .rename(columns={'Transaction_ID': TARGET})\n",
    "    )\n",
    "    panel = panel.sort_values([ID_COL, DATE_COL])\n",
    "    return panel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 3. DATA INSPECTION\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df: pd.DataFrame):\n",
    "    print(df.head())\n",
    "    print(df.info())\n",
    "    print(df.describe())\n",
    "    # Plot a sample series\n",
    "    sample = df[ID_COL].unique()[0]\n",
    "    sns.lineplot(data=df[df[ID_COL] == sample], x=DATE_COL, y=TARGET)\n",
    "    plt.title(f\"{TARGET} over time for SKU {sample}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 4. PREPROCESSING & RESAMPLING\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, freq: str = 'D') -> pd.DataFrame:\n",
    "    # Ensure continuous date index per SKU\n",
    "    df = df.set_index(DATE_COL)\n",
    "    out = []\n",
    "    for sku, grp in df.groupby(ID_COL):\n",
    "        grp = grp.resample(freq).asfreq()\n",
    "        # Interpolate missing target\n",
    "        grp[TARGET] = grp[TARGET].interpolate(method='time')\n",
    "        # Forward-fill exogenous\n",
    "        grp[EXOG_FEATURES] = grp[EXOG_FEATURES].ffill()\n",
    "        grp[ID_COL] = sku\n",
    "        out.append(grp)\n",
    "    result = pd.concat(out).reset_index()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 5. FEATURE ENGINEERING\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df: pd.DataFrame, lags: list = [1,7,14], rolling_windows: list = [7,14]):\n",
    "    df = df.copy()\n",
    "    # Lag features\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby(ID_COL)[TARGET].shift(lag)\n",
    "    # Rolling window stats\n",
    "    for window in rolling_windows:\n",
    "        grp = df.groupby(ID_COL)[TARGET]\n",
    "        df[f'rmean_{window}'] = grp.shift(1).rolling(window).mean().reset_index(level=0, drop=True)\n",
    "        df[f'rstd_{window}'] = grp.shift(1).rolling(window).std().reset_index(level=0, drop=True)\n",
    "    # Drop NaNs from new features\n",
    "    df = df.dropna()\n",
    "    # Feature list\n",
    "    FEATURES = [c for c in df.columns if c not in [ID_COL, DATE_COL, TARGET]]\n",
    "    return df, FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 6. TRAIN/TEST SPLIT\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_time(df: pd.DataFrame, test_size: float = 0.2):\n",
    "    train_parts, test_parts = [], []\n",
    "    for sku, grp in df.groupby(ID_COL):\n",
    "        n = len(grp)\n",
    "        split = int(n * (1 - test_size))\n",
    "        train_parts.append(grp.iloc[:split])\n",
    "        test_parts.append(grp.iloc[split:])\n",
    "    return pd.concat(train_parts), pd.concat(test_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 7. MODELING & CV\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(train_df: pd.DataFrame, FEATURES: list, param_grid: dict,\n",
    "           cv_splits: int = 5, randomized: bool = True, n_iter: int = 20):\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "    model = lgb.LGBMRegressor(objective='regression')\n",
    "    if randomized:\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,\n",
    "            cv=tscv,\n",
    "            scoring=custom_scorer,\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=tscv,\n",
    "            scoring=custom_scorer,\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    search.fit(train_df[FEATURES], train_df[TARGET])\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "    print(\"Best score:\", search.best_score_)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 8. FORECAST & EVALUATION\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_and_evaluate(model, test_df: pd.DataFrame, FEATURES: list):\n",
    "    preds = model.predict(test_df[FEATURES])\n",
    "    y_true = test_df[TARGET]\n",
    "    mae = mean_absolute_error(y_true, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, preds))\n",
    "    print(f\"Test MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n",
    "    # Plot per SKU or aggregate\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(test_df[DATE_COL], y_true, label='Actual')\n",
    "    plt.plot(test_df[DATE_COL], preds, label='Forecast')\n",
    "    plt.title('Forecast vs Actual')\n",
    "    plt.legend(); plt.show()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# 9. MAIN PIPELINE\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load & aggregate\n",
    "    df_raw = load_and_aggregate('final_total_clean.xlsx')\n",
    "    explore_data(df_raw)\n",
    "\n",
    "    # Preprocess\n",
    "    df_prep = preprocess(df_raw, freq='D')\n",
    "\n",
    "    # Features\n",
    "    df_feat, FEATURES = create_features(df_prep)\n",
    "\n",
    "    # Split\n",
    "    train_df, test_df = train_test_split_time(df_feat, test_size=0.2)\n",
    "\n",
    "    # Hyperparameters\n",
    "    param_grid = {\n",
    "        'num_leaves': [31, 61, 127],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 300, 500]\n",
    "    }\n",
    "\n",
    "    # CV & Fit\n",
    "    cv_model = run_cv(train_df, FEATURES, param_grid, cv_splits=5)\n",
    "\n",
    "    # Forecast & Evaluate\n",
    "    preds = forecast_and_evaluate(cv_model, test_df, FEATURES)\n",
    "\n",
    "    # Save results\n",
    "    model_path = 'orders_forecast_model.joblib'\n",
    "    import joblib; joblib.dump(cv_model, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
